{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting kagglehub\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/e7/71927b088047132317c14eb513d69c8375ddba3c9029d4154a054f6c8765/kagglehub-0.2.9-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: packaging in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from kagglehub) (2.27.1)\n",
      "Requirement already satisfied: tqdm in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from kagglehub) (4.66.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from requests->kagglehub) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from requests->kagglehub) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from requests->kagglehub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: colorama in d:\\miniconda\\envs\\py3.8\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.8)\n",
      "Path to dataset files: C:\\Users\\zhang\\.cache\\kagglehub\\datasets\\mczielinski\\bitcoin-historical-data\\versions\\154\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mczielinski/bitcoin-historical-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 6772281\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Create class BitcoinDataset which is a child of Dataset class from torch.util.data\n",
    "class BitcoinDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file.\n",
    "        \"\"\"\n",
    "        # Load data from CSV, select columns and drop null values\n",
    "        self.dataframe = pd.read_csv(csv_file)[['High','Low','Open','Close']].dropna()\n",
    "\n",
    "        # Extract the features for easier manipulation\n",
    "        self.features = self.dataframe.values\n",
    "\n",
    "        # Calculate mean and std for normalization\n",
    "        self.mean = self.features.mean(axis=0)\n",
    "        self.std = self.features.std(axis=0)\n",
    "\n",
    "        # Apply normalization to features\n",
    "        self.features = (self.features - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Return the item at index idx in the form of tensor\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "# Create an instance of BitcoinDataset and store in variable\n",
    "csv_file = './datasets/btcusd_1-min_data.csv'\n",
    "dataset = BitcoinDataset(csv_file)\n",
    "print(\"Number of samples in dataset:\", len(dataset))\n",
    "# Then, the batch_size and input_dim were set. The dataset was divided into train and test datasets. Each of these was loaded into a DataLoader. The device was set to ‘cuda’ if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batchsize and input dimensions\n",
    "batch_size = 64\n",
    "input_dim = 4\n",
    "\n",
    "# Split dataset into train and test in the ratio of 80:20\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8,0.2])\n",
    "\n",
    "# Use DataLoader for batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define Device as Cuda if available else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=40, latent_dim=3, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # Latent mean and variance\n",
    "        self.mean_layer = nn.Linear(latent_dim, 1)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "     \n",
    "    # Encode function\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "    # Add Reparameterization\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    # Decode function\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # Forward Function\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "    # Reconstruct input from compressed form\n",
    "    def reconstruction(mean, log_var):\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    # Reproduction Loss\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    \n",
    "    # KL Divergence Loss\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "# VAE Model created and stored in device\n",
    "model = VAE().to(device)\n",
    "\n",
    "# Optimizer defined\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  0.0010465322552025735\n",
      "\tEpoch 2 \tAverage Loss:  6.398167515173866e-05\n",
      "\tEpoch 3 \tAverage Loss:  4.211362657392762e-05\n",
      "\tEpoch 4 \tAverage Loss:  2.9124277026888643e-05\n",
      "\tEpoch 5 \tAverage Loss:  2.362720924003563e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128.00730520299112"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        \n",
    "        # Iterate over the batches formed by DataLoader\n",
    "        for batch_idx, x in enumerate(train_dataloader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Reset Gradient\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            \n",
    "            # Calculate batch loss and then overall loss\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            # Backpropagate the loss and train the optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "train(model, optimizer, epochs=5, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
